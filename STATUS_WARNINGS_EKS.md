# Verifica√ß√£o do Status do Deploy EKS

## Status Atual

Voc√™ est√° vendo warnings **NORMAIS** durante o deploy inicial do cluster EKS:

### ‚ö†Ô∏è Warning 1: AWS Load Balancer Controller
```
0/1 nodes are available: 1 node(s) didn't have free ports for the requested pod ports
```

**O que significa**: O pod precisa de portas espec√≠ficas no node (hostPort), mas:
- O node ainda n√£o est√° completamente pronto, OU
- Outro pod est√° tentando usar as mesmas portas

**√â normal?**: ‚úÖ SIM - durante os primeiros 10-15 minutos do deploy

### ‚ö†Ô∏è Warning 2: CoreDNS
```
no nodes available to schedule pods
```

**O que significa**: N√£o h√° nodes prontos ainda para agendar os pods do CoreDNS

**√â normal?**: ‚úÖ SIM - significa que o Node Group ainda est√° sendo provisionado

## üïê Timeline Esperada do Deploy

| Tempo | Etapa | Status Esperado |
|-------|-------|-----------------|
| 0-10 min | EKS Cluster | Criando control plane |
| 10-15 min | Node Group | Provisionando inst√¢ncias SPOT |
| 15-20 min | Nodes | Registrando no cluster |
| 20-25 min | Sistema | CoreDNS iniciando |
| 25-30 min | Addons | Load Balancer Controller instalando |

## ‚úÖ Comandos para Verificar o Progresso

### 1. Verificar Nodes
```bash
kubectl get nodes -o wide
```

**O que esperar**:
- Inicialmente: "No resources found"
- Depois: 1 node com status "NotReady"
- Finalmente: 1 node com status "Ready"

### 2. Verificar Pods do Sistema
```bash
kubectl get pods -n kube-system
```

**O que esperar**:
```
NAME                                            READY   STATUS
aws-node-xxxxx                                  2/2     Running
coredns-xxxxx                                   1/1     Running
kube-proxy-xxxxx                                1/1     Running
aws-load-balancer-controller-xxxxx              1/1     Running
```

### 3. Verificar Events
```bash
kubectl get events -n kube-system --sort-by='.lastTimestamp' | tail -20
```

**Warnings normais durante deploy**:
- FailedScheduling (nodes n√£o prontos)
- ImagePullBackOff (tempor√°rio)
- Unhealthy (health checks falhando at√© pods iniciarem)

### 4. Status do Node Group (via AWS CLI)
```bash
aws eks describe-nodegroup \
  --cluster-name tech-challenge-eks \
  --nodegroup-name tech-challenge-nodes \
  --region us-east-1 \
  --query 'nodegroup.status'
```

**Estados poss√≠veis**:
- CREATING ‚Üí ACTIVE (esperado)
- DEGRADED (problema!)

### 5. Verificar Inst√¢ncias EC2 SPOT
```bash
aws ec2 describe-instances \
  --filters "Name=tag:Name,Values=tech-challenge-eks-nodes" \
  --region us-east-1 \
  --query 'Reservations[*].Instances[*].[InstanceId,State.Name,InstanceType]' \
  --output table
```

## üîß Quando os Warnings s√£o um Problema Real?

Os warnings s√£o **normais** se:
- ‚úÖ Voc√™ est√° nos primeiros 15-20 minutos do deploy
- ‚úÖ Comando `kubectl get nodes` mostra nodes em "NotReady" ou n√£o mostra nada ainda
- ‚úÖ Os warnings s√£o sobre FailedScheduling ou "no nodes available"

Os warnings s√£o **um problema** se:
- ‚ùå Passaram mais de 30 minutos
- ‚ùå Nodes aparecem como "Ready" mas pods continuam "Pending"
- ‚ùå Pods est√£o em "CrashLoopBackOff" ou "ImagePullBackOff" por muito tempo
- ‚ùå Node Group est√° em estado "DEGRADED"

## üö® Troubleshooting (se necess√°rio ap√≥s 30 min)

### Problema: Nodes n√£o ficam prontos

```bash
# Ver logs do kubelet no node
kubectl describe node <node-name>

# Verificar eventos
kubectl get events -A --sort-by='.lastTimestamp'
```

**Poss√≠veis causas**:
- Inst√¢ncia SPOT n√£o dispon√≠vel (AWS recusou)
- Problemas de rede (NAT Gateway, Internet Gateway)
- Problemas de IAM (LabRole sem permiss√µes)

### Problema: Load Balancer Controller n√£o inicia

```bash
# Ver logs do controller
kubectl logs -n kube-system deployment/aws-load-balancer-controller

# Verificar se o service account existe
kubectl get serviceaccount -n kube-system aws-load-balancer-controller
```

**Poss√≠veis causas**:
- Conflito de portas (outro pod usando hostPort)
- Imagem n√£o baixada (ImagePullBackOff)
- Recursos insuficientes no node

## üí° Solu√ß√£o se persistir ap√≥s 30 minutos

### Op√ß√£o 1: Aumentar n√∫mero de nodes (tempor√°rio)
```bash
aws eks update-nodegroup-config \
  --cluster-name tech-challenge-eks \
  --nodegroup-name tech-challenge-nodes \
  --scaling-config desiredSize=2,minSize=1,maxSize=2 \
  --region us-east-1
```

### Op√ß√£o 2: Remover hostPort do Load Balancer Controller

Se o problema for especificamente com hostPort, podemos reconfigurar o Helm chart para n√£o usar hostPort (isso √© seguro para ambientes de desenvolvimento).

## üìä Monitoramento em Tempo Real

```bash
# Assistir nodes ficarem prontos
watch -n 5 'kubectl get nodes'

# Assistir pods do sistema
watch -n 5 'kubectl get pods -n kube-system'

# Assistir todos os eventos
kubectl get events -A -w
```

## ‚úÖ Conclus√£o

**AGUARDE mais 10-15 minutos.** Os warnings que voc√™ est√° vendo s√£o **completamente normais** durante o deploy inicial do EKS. 

O processo est√° funcionando conforme esperado:
1. ‚úÖ Terraform aplicou com sucesso
2. üîÑ EKS Cluster est√° criado
3. üîÑ Node Group est√° provisionando inst√¢ncias SPOT
4. ‚è≥ Pods est√£o aguardando nodes ficarem prontos

Quando tudo estiver pronto, voc√™ ver√°:
```bash
$ kubectl get nodes
NAME                         STATUS   ROLES    AGE   VERSION
ip-10-0-x-x.ec2.internal    Ready    <none>   5m    v1.31.x

$ kubectl get pods -n kube-system
NAME                                            READY   STATUS    RESTARTS   AGE
aws-load-balancer-controller-xxxxx              1/1     Running   0          3m
coredns-xxxxx                                   1/1     Running   0          5m
```

**Volte a verificar em 10-15 minutos!** üöÄ
